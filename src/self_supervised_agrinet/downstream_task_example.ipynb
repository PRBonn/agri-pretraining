{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Task Example\n",
    "\n",
    "In this guide we'll give you all the information needed to implement a downstream task and use our pre-training (or the pre-training obtained from our code with your data). \n",
    "Every section explains one file and give a small example of what is important to implement to make everything work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataloader\n",
    "\n",
    "Implement a DataModule. The class should be able to get the images and the annotations needed for the training. It needs to have a __ __getitem__ __ method which returns a single sample of the dataset.\n",
    "\n",
    "Here an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        \"\"\" Create dataset. \"\"\"\n",
    "        path_to_split_info = self.cfg['data']['split']\n",
    "\n",
    "        with open(path_to_split_info) as istream:\n",
    "          split_info = yaml.safe_load(istream)\n",
    "\n",
    "        path_to_dataset = self.cfg['data']['path_to_dataset']\n",
    "\n",
    "        train_filenames = split_info['train']\n",
    "        \n",
    "        # similarly you can define a dataset for the test and validation set\n",
    "        self._uav_train = UAVDataset(path_to_dataset,\n",
    "                                     train_filenames,\n",
    "                                     # if you need a pre processing step for images, transformations is the way to go\n",
    "                                     transformations=get_transformations(self.cfg)\n",
    "                                     )\n",
    "\n",
    "    # similarly you can define a dataloaser for the test and validation set\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        shuffle: bool = self.cfg['data']['shuffle_train']\n",
    "        batch_size: int = self.cfg['data']['batch_size_train']\n",
    "        n_workers: int = self.cfg['data']['num_workers']\n",
    "\n",
    "        loader = DataLoader(self._uav_train, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers)\n",
    "\n",
    "        return loader\n",
    "    \n",
    "\n",
    "class UAVDataset(Dataset):\n",
    "  \"\"\" Represents the UAV dataset. \"\"\"\n",
    "\n",
    "  def __init__(self, path_to_dataset: str, filenames: List[str], transformations: List[Transformation]):\n",
    "    super().__init__()\n",
    "\n",
    "    # get path to all RGB images\n",
    "    self.path_to_images = path_to_dataset + ...\n",
    "\n",
    "    self.image_files: List[str] = []\n",
    "    for fname in os.listdir(self.path_to_images):\n",
    "        if fname in filenames:\n",
    "            self.image_files.append(fname)\n",
    "\n",
    "    # get path to all ground-truth semantic masks\n",
    "    self.path_to_annos = path_to_dataset + ...\n",
    "\n",
    "    self.anno_files: List[str] = []\n",
    "    for fname in os.listdir(self.path_to_annos):\n",
    "        if fname in filenames:\n",
    "            self.anno_files.append(fname)\n",
    "\n",
    "    # specify image transformations\n",
    "    self.img_to_tensor = transforms.ToTensor()\n",
    "    self.transformations = transformations\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, str]]:\n",
    "        \"\"\" Get a single sample of the dataset. \"\"\"\n",
    "\n",
    "        # get the current image\n",
    "        path_to_current_img: str = os.path.join(self.path_to_images, self.image_files[idx])\n",
    "        img_pil = Image.open(path_to_current_img)  \n",
    "        img = self.img_to_tensor(img_pil)\n",
    "\n",
    "        # get the corresponding annotation\n",
    "        path_to_current_anno: str = os.path.join(self.path_to_annos, self.anno_files[idx])\n",
    "        anno = np.array(Image.open(path_to_current_anno))  \n",
    "        anno = torch.Tensor(anno)\n",
    "\n",
    "        # apply a set of transformations to the raw_image, image and anno\n",
    "        for transformer in self.transformations:\n",
    "            img_pil, img, anno = transformer(img_pil, img, anno)\n",
    "\n",
    "        return {'img': img_pil, 'data': img, 'anno': anno, 'fname': self.image_files[idx]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "The model should have as encoder ResNet50 if you want to use our pre-training code. Our methodology can work with any backbone, if you change something in our pre-training architecture be sure to include the same modifications here. The model needs a decoder, specialized for your downstream task - remember to check the dimensions of your encoder output before designing your decoder. \n",
    "\n",
    "Here an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_size, dropout, ...): # pass all the information needed for your decoder\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.encoder = Encoder(in_size = self.in_size, dropout = dropout)\n",
    "        self.decoder = Decoder(...)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        output = self.encoder(input)\n",
    "        return self.decoder(output)\n",
    "    \n",
    "    # define your training, validation and test step\n",
    "    def training_step(self,...)\n",
    "    \n",
    "    # define your loss\n",
    "    def compute_loss(self,...)\n",
    "\n",
    "    # define your optimization method\n",
    "    def configure_optimizers(self,...):\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, dropout, OS = 32, bn_d = 0.1, model = 'resnet50'):\n",
    "        super().__init__()\n",
    "        self.net = ResNet(in_size, OS, dropout, bn_d, model)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # this are the standard sizes of the projector network after ResNet50\n",
    "        sizes = [2048, 8192, 8192, 8192]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n",
    "            layers.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # define your forward pass\n",
    "        # you can use the whole encoder up to the final embedding\n",
    "        # or you can run each layer individually until the depth you like\n",
    "        skips = {}\n",
    "        os = 1\n",
    "        \n",
    "        x, skips, os = self.net.run_layer(input, self.net.conv1, skips, os)\n",
    "        x, skips, os = self.net.run_layer(x, self.net.bn1, skips, os)\n",
    "        x, skips, os = self.net.run_layer(x, self.net.relu, skips, os)\n",
    "        x, skips, os = self.net.run_layer(x, self.net.maxpool, skips, os)\n",
    "        x, skips, os = ...\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        # define your decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        # define your decoder forward pass\n",
    "        output = input\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Config file\n",
    "\n",
    "Use a config file to store all important variables for your network, as data path, batch size, type of architecture, details for the decoder. Usually the config file is a yaml file, you can find the config file we use for the pre-training in this repository and use it as example.\n",
    "\n",
    "## 4. Main\n",
    "\n",
    "Implement the main.py file. \n",
    "\n",
    "Remember to load the weights of the pre training as `model.load_state_dict(checkpoint['state_dict'])`. \n",
    "If you implement the model exactly as in our pre-training backbone everything should work easily. If this is not the case, the most common reason is that the keys of the two dictionaries do not match -- layers and parts of the architecture have different names. One way to solve the issue is to re-name the keys of the loaded dictionary to match yours. For example:\n",
    "\n",
    "`real_checkpoint = {}\n",
    " for k in checkpoint['state_dict'].keys():\n",
    "     new_string = k.replace('model.','').replace('net.','').replace('resnet50','net')\n",
    "     real_checkpoint[new_string] = checkpoint['state_dict'][k]\n",
    "`\n",
    "\n",
    "which get a key `model.net.resnet50.layer` and transforms it into `net.layer`. \n",
    "\n",
    "Once you loaded the pre-trained weights, you can define everything else as usual and fit / test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
